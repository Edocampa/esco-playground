{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Without GPU this is quite slow. Further info on using a GCP server on GPU can be found in GCP.md\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ESCO data from SPARQL and create a dataframe with some useful columns.\n",
    "# Alternatively, use the pre-processed esco.json file.\n",
    "df = pd.read_json(\"../esco/esco.json.gz\", orient=\"records\")\n",
    "df.index = df.s\n",
    "skills = df.groupby(df.s).agg(\n",
    "    {\n",
    "        \"altLabel\": lambda x: list(set(x)),\n",
    "        \"label\": lambda x: x.iloc[0],\n",
    "        \"description\": lambda x: x.iloc[0],\n",
    "        \"skillType\": lambda x: x.iloc[0],\n",
    "    }\n",
    ")\n",
    "# Add a lowercase text field for semantic search.\n",
    "skills[\"text\"] = skills.apply(\n",
    "    lambda x: \"; \".join([x.label] + x.altLabel + [x.description]).lower(), axis=1\n",
    ")\n",
    "# .. and a set of all the labels for each skill.\n",
    "skills[\"allLabel\"] = skills.apply(\n",
    "    lambda x: {t.lower() for t in x.altLabel} | {x.label.lower()}, axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test some skills\n",
    "list(skills[skills.label.str.contains(\"Python\")].altLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pattern(kn: dict):\n",
    "    \"\"\"Given an ESCO skill entry in the dataframe, create a pattern for the matcher.\n",
    "    \n",
    "    The entry has the following fields:\n",
    "    - label: the preferred label\n",
    "    - altLabel: a list of alternative labels\n",
    "    - the skillType: e.g. knowledge, skill, ability\n",
    "\n",
    "    The logic uses some euristic to decide whether to use the preferred label or the alternative labels.\n",
    "    \"\"\"\n",
    "    label = kn[\"label\"]    \n",
    "    pattern = [{\"LOWER\": label.lower()}] if len(label) > 3 else [{\"TEXT\": label}] \n",
    "    patterns = [pattern]\n",
    "    altLabel = [kn['altLabel']] if isinstance(kn['altLabel'], str) else kn['altLabel']\n",
    "    for alt in altLabel:\n",
    "        if len(alt) <= 3:\n",
    "            candidate = [{\"TEXT\": alt}]\n",
    "        elif len(alt.split()) > 1:\n",
    "            candidate = [{\"LOWER\": x} for x in alt.lower().split()]\n",
    "        else:\n",
    "            candidate = [{\"LOWER\": alt.lower()}]\n",
    "        if candidate in patterns:\n",
    "            print(f\"Skipping {candidate}\")\n",
    "            continue\n",
    "        patterns.append(candidate)\n",
    "    pattern_identifier = f\"{kn['skillType'][:2]}_{label.replace(' ', '_')}\".upper().translate(\n",
    "        str.maketrans(\"\", \"\", \"()\")\n",
    "    )\n",
    "    if 'python' in pattern_identifier.lower():\n",
    "        print(pattern_identifier, patterns)\n",
    "    return pattern_identifier, patterns\n",
    "\n",
    "# Create the patterns for the matcher\n",
    "m = dict(make_pattern(kni) for kni in skills.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy matcher with a blank model to validate the patterns.\n",
    "# If this doesn't work, spacy will raise an error.\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "nlp_test = spacy.blank(\"en\")\n",
    "m1 = Matcher(nlp_test.vocab, validate=True)\n",
    "for pid, patterns in m.items():\n",
    "    m1.add(pid, patterns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 3 patterns\n",
    "list(m.items())[:3]\n",
    "json.dump(m, open(\"../generated/esco_matchers.json\", \"w\"))\n",
    "esco_p = [{\"label\":\"ESCO\", \"pattern\": pattern } for k, p in m.items()  for pattern in p ]\n",
    "\n",
    "# Save the patterns to a json\n",
    "import json\n",
    "with open(\"../generated/esco_patterns.json\", \"w\") as f:\n",
    "    json.dump(esco_p, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an ESCO spacy entity recognizer model\n",
    "\n",
    "This entity recognizer reuses the en_core_web_trf model, that is quite good at identifying PRODUCTS.\n",
    "We will add a new entity label, ESCO, that uses the altLabel patterns to identify further entities.\n",
    "\n",
    "The ESCO entity label is added to the pipeline after the NER component, so that the NER component can identify the entities that are already in the en_core_web_trf model, and the ESCO component can add the ESCO entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is quite good at recognizing ICT entities like products.\n",
    "import spacy\n",
    "from spacy import displacy  # Load a viewer.\n",
    "nlp_e = spacy.load(\"en_core_web_trf\")\n",
    "ruler = nlp_e.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "ruler.add_patterns(esco_p)\n",
    "nlp_e.to_disk(\"../generated/en_core_web_trf_esco_ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from hashlib import sha256\n",
    "DATADIR = Path(sys.path[0]).parent / \"tests\" / \"data\"\n",
    "text = (DATADIR / \"rpolli.txt\").read_text()\n",
    "\n",
    "\n",
    "def get_stats(doc):\n",
    "    doc_id = sha256(doc.text.encode(\"utf-8\")).hexdigest()\n",
    "    all_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    ent_count = len(all_entities)\n",
    "    ent_unique = set(all_entities)\n",
    "    ent_unique_count = len(ent_unique)\n",
    "    ent_text_unique = set(text for text, _ in ent_unique)\n",
    "    ent_text_unique_count = len(ent_text_unique)\n",
    "    ent_skills = set((t, l) for t, l in ent_unique if l in (\"ESCO\", \"PRODUCT\"))\n",
    "    ent_skills_text_unique = len(set(text for text, _ in ent_skills))\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"ent_count\": ent_count,\n",
    "        \"ent_unique\": list(ent_unique),\n",
    "        \"ent_unique_count\": ent_unique_count,\n",
    "        \"ent_text_unique\": list(ent_text_unique),\n",
    "        \"ent_text_unique_count\": ent_text_unique_count,\n",
    "        \"ent_skills\": list(ent_skills),\n",
    "        \"ent_skills_text_unique\": ent_skills_text_unique,\n",
    "    }\n",
    "\n",
    "doc = nlp_e(text)\n",
    "# Show some stats\n",
    "get_stats(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the result.\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "The recognizer is tested processing a set of CVs and returning the entities found in each CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\".\").parent / \"tests\" / \"data\"\n",
    "cvs = yaml.safe_load((DATADIR / 'curricula.yaml').read_text(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for the test.\n",
    "\n",
    "\n",
    "def model_factory(model_name, patterns=None, config=None):\n",
    "    model = spacy.load(model_name)\n",
    "    if patterns:\n",
    "        config = config or {}\n",
    "        ruler = model.add_pipe(\"entity_ruler\", **config)\n",
    "        ruler.add_patterns(esco_p)\n",
    "    return model\n",
    "\n",
    "\n",
    "testcases = {\n",
    "    \"base\": model_factory(\"en_core_web_md\"),\n",
    "    \"trf\": model_factory(\"en_core_web_trf\"),\n",
    "    \"trf_pre\": model_factory(\"en_core_web_trf\", esco_p, config={\"before\": \"ner\"}),\n",
    "    \"trf_post\": model_factory(\"en_core_web_trf\", esco_p, config={\"after\": \"ner\"}),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testcases and save each result asap, since the test can take a long time.\n",
    "results_out = Path(\"results.out\")\n",
    "with results_out.open(\"wb\") as fh:\n",
    "    fh.write(b\"[\")\n",
    "    for model_name, nlp_model in testcases.items():\n",
    "        for doc, cv in nlp_model.pipe([(cv[\"text\"], cv) for cv in cvs], as_tuples=True):\n",
    "            stats = get_stats(doc)\n",
    "            stats[\"model\"] = model_name\n",
    "            fh.write(json.dumps(stats).encode())\n",
    "            fh.write(b\",\\n\")\n",
    "    fh.write(b\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Path(\"results.out\").read_text()\n",
    "data=yaml.safe_load(results)\n",
    "df = pd.DataFrame(data)\n",
    "df[\"model\"] = df.index // 9\n",
    "# aggregate the dataframe above by doc_id, using the couple (model, ent_.._count) as columns\n",
    "# and the doc_id as the index\n",
    "results = df.groupby([\"doc_id\", \"model\"]).agg(list).unstack()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "import re\n",
    "\n",
    "DATADIR = Path(sys.path[0]).parent / \"tests\" / \"data\"\n",
    "\n",
    "nlp_esco = spacy.load(\"../generated/en_core_web_trf_esco_ner/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = (DATADIR / \"rpolli.txt\").read_text()\n",
    "text = re.sub('\\n+','\\n',text_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp_esco(text)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
